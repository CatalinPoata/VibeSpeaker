# VibeSpeaker

## ğŸ¤ Speech Emotion Recognition (SER) using Multimodal Deep Learning

VibeSpeaker is a **multimodal Speech Emotion Recognition (SER) system** that integrates **audio, text, and spectrogram features** to accurately classify and analyze emotions in speech. The model leverages cutting-edge **deep learning architectures** like **WavLM, RoBERTa, and CNNs**, enabling real-time emotion recognition.

## ğŸš€ Features
- **Multimodal Learning**: Uses **audio features (WavLM)**, **text features (RoBERTa)**, and **visual features (spectrograms via CNNs)**.
- **Speech-to-Text Integration**: Employs **Whisper-large-v3-turbo** for real-time transcription.
- **Robust Preprocessing**: Normalization, resizing, and feature extraction from different modalities.
- **Emotion Classification & Regression**: Supports **8 emotion categories** and **continuous emotion prediction (valence, arousal, dominance)**.
- **Fine-tuned Transformer Models**: Optimized for improved performance on multilingual datasets.

## ğŸ› ï¸ Technologies Used
- **Audio Processing**: [WavLM (microsoft/wavlm-base-plus)](https://huggingface.co/microsoft/wavlm-base-plus)
- **Text Processing**: [RoBERTa (roberta-base)](https://huggingface.co/roberta-base) & [XLM-RoBERTa](https://huggingface.co/xlm-roberta-base)
- **Visual Features**: CNN-based spectrogram analysis
- **Speech-to-Text**: [Whisper-large-v3-turbo](https://openai.com/research/whisper)
- **Optimization**: AdamW optimizer, learning rate scheduling, gradient accumulation

## ğŸ“‚ Project Structure
```
VibeSpeaker/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dataset.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ demo.py
â””â”€â”€ README.md
```

## ğŸ–¼ï¸ Model Architecture
![Model Architecture](vibespeaker_fig_4_1.png)

## ğŸ”§ Installation & Usage
### 1ï¸âƒ£ Clone the Repository
```sh
git clone https://github.com/your-username/VibeSpeaker.git
cd VibeSpeaker
```
### 2ï¸âƒ£ Install Dependencies
```sh
pip install -r requirements.txt
```
### 3ï¸âƒ£ Run Inference on an Audio File
```sh
python inference.py --audio_file sample.wav
```

## ğŸ† Experimental Results
| Model       | Valence CCC | Arousal CCC | Dominance CCC | Accuracy |
|------------|------------|------------|--------------|----------|
| WavLM + RoBERTa + CNN  | **0.2809** | **0.3769** | **0.3249** | **95.95%** |
| WavLM + RoBERTa | 0.2754 | 0.3937 | 0.2947 | 86.64% |
| WavLM Only | 0.2354 | 0.3544 | 0.2183 | 54.66% |

### ğŸ“Š Performance Metrics
![Performance Metrics](vibespeaker_fig_12_1.png)

## âœï¸ Authors
- [ViÈ™an IonuÈ›](https://github.com/visan-ionut)
- [PoaÈ›Äƒ CÄƒtÄƒlin-Andrei](https://github.com/CatalinPoata)
- [Vulpe È˜tefan](https://github.com/stefanvulpe-dev)

ğŸ“ Developed as part of the **Artificial Intelligence Master, UNSTP Bucharest**.

## ğŸ“œ License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---
Feel free to contribute, report issues, or share ideas for improvements! ğŸš€
